# -*- coding: utf-8 -*-
"""BaseDeDatosGOV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HH3TW9ihLH3inkbkku16jaIU1Ebmk0nK
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import folium
from folium.plugins import HeatMap
from google.colab import files
from google.colab import output
import io

url = 'https://raw.githubusercontent.com/JuanJDCR/For-i-in-datacraks/refs/heads/main/Delitos_Informaticos_V1_20250714.csv'
data = pd.read_csv(url)
data

data.info()

"""#Fase De Limpieza

"""

data.shape

data.info()

#Eliminamos todos los NA
data = data.dropna()
data.info()

# Capitalizar valores
for col in data.select_dtypes(include='object').columns:
    data[col] = data[col].str.capitalize()

#leemos las variables categoricas
for col in data.select_dtypes(include=['object']).columns:
  print(f"Column: {col}")
  print(data[col].value_counts())
  print("-" * 20)

# Estadistica descriptiva
data.describe()

#Eliminando las filas repetidas
print(f'Tama√±o del set antes de eliminar las filas repetidas: {data.shape}')
data.drop_duplicates(inplace=True)
print(f'Tama√±o del set despu√©s de eliminar las filas repetidas: {data.shape}')

#No habian filas repetidas.

"""***Outliers* en las variables num√©ricas**


"""

# Boxplot para las variables num√©ricas (vertical)
for col in data.select_dtypes(include=['number']).columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=data[col])
    plt.title(f'Boxplot de {col}')
    plt.ylabel(col)
    plt.show()

    #Hay un registro cerca del 2010, debido a que el valor esta dos desviaciones estandar por fuera de los datos, habr√° que analizarlo para ver si es un registro real o algun outlier!!!

"""Para este momento todas las correcciones a la base de datos se realizaron y la version corregida pasar√° a ser un DataFrame en este codigo:"""

#Definimos el DataFrame
df = pd.DataFrame(data)
df

#Evaluando si un valor que pensamos que era outlier segun la grafica anterior en realidad es un dato correcto o no
contador = 0
for i in range(len(df)):
  if df['A√ëO_HECHOS'][i] == 2010:
    contador+=1
    print(df.loc[i])
    print(contador)
#Al parecer el a√±o 2010 se repite varias veces en el dataset y es el a√±o mas peque√±o, por lo que
# podemos afirmar que en las columnas con variables numericas no hay outliers!

"""#Despues de hacer la limpieza grafica, procedamos con la limpieza numerica y categorica

**Como sabemos que no hay errores tipograficos en los registros?**

**Como sabemos que no hay errores numericos en los registros?**

**¬øCuales de nuestras variables son categoricas y cuales son numericas?**
"""

for col in df.columns:
  if df[col].dtype == 'int64' or df[col].dtype == 'float64':
    print(df[col])

# Clasificaci√≥n
def arreglo_clasificacion(df):
  tipos = {
    "numericas": [],
    "categoricas": [],
    "alfanumericas": [],
    "fechas": [],
    "otros": []}
  for col in df.columns:
      tipo = df[col].dtype

      if pd.api.types.is_numeric_dtype(df[col]):
          tipos["numericas"].append(col)
      elif pd.api.types.is_categorical_dtype(df[col]) or df[col].nunique() < 20:
         tipos["categoricas"].append(col)
      elif pd.api.types.is_string_dtype(df[col]):
          if df[col].str.contains(r'\d', regex=True).any():  # contiene n√∫meros y letras
              tipos["alfanumericas"].append(col)
          else:
              tipos["categoricas"].append(col)
      elif pd.api.types.is_datetime64_any_dtype(df[col]):
          tipos["fechas"].append(col)
      else:
          tipos["otros"].append(col)
    # Mostrar resultados
  return tipos
  for categoria, columnas in tipos.items():
      print(f"{categoria.upper()}: {columnas}")

arreglo_clasificacion(df)

"""**Arreglando la categorizacion**:

** (dtype:"OBJECT")*
"""

#El sistema reconoce a a√±o denuncia y a√±o entrada como variables categoricas (dtype:"OBJECT") por que
#en algunas filas hay valores de estas columnas entre comillas o hay algunos registros con cosas que no son numeros

print(df["A√ëO_DENUNCIA"])
print(df["A√ëO_ENTRADA"])

"""**Convirtiendo todas las columnas con numericos a verdaderos numericos**

¬øQue son numericos y verdaderos numericos en este contexto?

Los numericos son columnas que en su mayoria tienen numeros pero pueden haber errores como s/d o Na

Los verdaderos numericos son columnas de datos que siguen teniendo los mismos numeros pero cualquier error ha sido convertido a "NaN", y asi, todos los posibles errores han sido normalizados para su posterior analisis

Los numericos son columnas que en su mayoria tienen numeros pero pueden haber errores como s/d o Na

*   errors="coerce" convierte cualquier valor inv√°lido (por ejemplo "S/D", "NA") en NaN

*   El tipo final ser√° float64 (porque NaN no existe para int, aunque sean a√±os)


"""

#Convirtiendo en el DataFrame original los valores numericos a verdaderos numericos
df["A√ëO_ENTRADA"] = pd.to_numeric(df["A√ëO_ENTRADA"], errors="coerce")
df["A√ëO_DENUNCIA"] = pd.to_numeric(df["A√ëO_DENUNCIA"], errors="coerce")
df["A√ëO_HECHOS"] = pd.to_numeric(df["A√ëO_DENUNCIA"], errors="coerce")
df["TOTAL_PROCESOS"] = pd.to_numeric(df["A√ëO_DENUNCIA"], errors="coerce")

# Y si volvemos a aplicar la categorizacion:

arreglo_clasificacion(df)

#Notemos que la clasificacion ahora si introduce columnas ya modificadas a numericos verdaderos a la clave de columnas numericas.

data

"""*La informacion que contiene el dataset es realmente fiable?*"""

cols_cat = ['CRIMINALIDAD', 'ESTADO', 'ETAPA_CASO', 'PA√çS_HECHO', 'DEPARTAMENTO_HECHO', 'MUNICIPIO_HECHO', 'SECCIONAL', 'DELITO', 'GRUPO_DELITO', 'CONSUMADO']
for col in cols_cat:
  print(f'Columna {col}: {data[col].nunique()} subniveles' )

"""**Cuantos subniveles hay y cuantas veces se repiten cada uno?**

"""

for alpha in cols_cat:
    print("-"*200)
    print(f"La columna: {alpha} contiene los siguientes datos: {data[alpha].unique()}")

"""TAREA PERSONAL!!

Terminar de arreglar la categorizacion de los datos y descubrir los errores tipograficos

*Arreglando los errores tipograficos de mi dataset*

#En este codigo intent√© hacer una funcion que me corrigiera los errores de las variables categoricas
"""

#NO USAR, SOLO ANALIZAR.
import unicodedata
arreglo_clasificacion(df)

#Una vez conocidas las variables categoricas, una lista con las variables ya categorizadas es:

columnas_categoricas = ['CRIMINALIDAD', 'ESTADO', 'ETAPA_CASO', 'PA√çS_HECHO', 'DEPARTAMENTO_HECHO', 'MUNICIPIO_HECHO', 'SECCIONAL', 'DELITO', 'GRUPO_DELITO', 'CONSUMADO']

def analizar_errores_tipograficos(df, columnas_categoricas):
    for col in columnas_categoricas:
        print(f"\n Columna: {col}")

        # Crear columna limpia temporal
        serie_limpia = (
            df[col]
            .astype(str)                     # Asegura string
            .str.strip()                     # Quita espacios
            .str.lower()                     # Pone en min√∫sculas
            .str.normalize('NFKD')           # Quita acentos - #PROBLEMA, NO ME QUITO TODOS LOS ACENTOS.
            .str.encode('ascii', errors='ignore')  # Quita s√≠mbolos raros
            .str.decode('utf-8'))           # Decodifica


        # Agrupar las versiones originales por su forma "normalizada"
        agrupado = pd.DataFrame({
            "original": df[col],
            "normalizado": serie_limpia
        }).groupby("normalizado")["original"].unique() #Este groupby (Funcion de pandas) se encarga de agrupar las dos claves del diccionario convertido a dataframe
        #"AGRUPADO" y ver cuales son sus subniveles


        # Mostrar resultados si hay m√°s de una versi√≥n original para un mismo valor normalizado
        contadore = 0
        for normalizado, originales in agrupado.items(): #Este ciclo se encarga de iterar sobre el dataframe "agrupado" y despues
            contadore += 1
            if len(originales) > 1:
                print(f"Posible error tipogr√°fico ‚Üí '{normalizado}' : {list(originales)}")
                print(contadore)
                print(df.iloc[contadore])
                for i in range(len(df)):
                  if i == contadore:
                    for a in df.columns:
                      if a in columnas_categoricas == True:
                        for b in df[a]:
                          if isinstance(b, str):
                            texto_normalizado = unicodedata.normalize('NFD', b)
                            texto_sin_tildes = ''.join(
                            c for c in texto_normalizado if unicodedata.category(c) != 'Mn')
                            df[b] == (texto_sin_tildes, inplace == True)
                            print(df[b])











analizar_errores_tipograficos(df, columnas_categoricas)

"""##Codigo funcional:"""

import unicodedata
from collections import defaultdict

# Funci√≥n para quitar tildes, conservar la √±
valores_columnas = []
for i in df.columns:
  valores_columnas.append(df[i])

def quitar_tildes_conservando_enie(valores_columnas):
    if not isinstance(valores_columnas, str):
        return valores_columnas
    valores_columnas = valores_columnas.replace('√±', '__enie__').replace('√ë', '__ENIE__')
    valores_columnas = unicodedata.normalize('NFD', valores_columnas)
    valores_columnas = ''.join(c for c in valores_columnas if unicodedata.category(c) != 'Mn')
    return valores_columnas.replace('__enie__', '√±').replace('__ENIE__', '√ë')

quitar_tildes_conservando_enie(valores_columnas)

# Funci√≥n principal
def detectar_y_cambiar_errores(df, columnas_categoricas):
    for col in columnas_categoricas:
        print(f"\nüîç Analizando columna: {col}")

        # Normalizaci√≥n
        col_original = df[col].astype(str)
        col_limpia = col_original.str.strip().str.lower().apply(quitar_tildes_conservando_enie)

        # Construir mapa de normalizado ‚Üí [originales]
        normalizado_a_originales = defaultdict(list)
        for original, limpio in zip(col_original, col_limpia):
            normalizado_a_originales[limpio].append(original)

        # Crear mapa de reemplazo: normalizado ‚Üí versi√≥n m√°s frecuente
        reemplazos = {}
        for normalizado, lista in normalizado_a_originales.items():
            frecuencia = pd.Series(lista).value_counts()
            version_oficial = frecuencia.idxmax()
            if len(frecuencia) > 1:
                print(f"‚ö†Ô∏è Posible error tipogr√°fico: {list(frecuencia.index)} ‚Üí Se usar√°: '{version_oficial}'")
            reemplazos.update({val: version_oficial for val in frecuencia.index})

        # Aplicar correcci√≥n
        df[col] = df[col].map(reemplazos)

    print("\n‚úÖ Limpieza completada.")
    return df
detectar_y_cambiar_errores(df, columnas_categoricas)


#Aplicandolo al dataframe original:
contadore = 0
for col in df.columns:
  contadore+=1
  if df[col].dtype == 'object':
    df[col] = df[col].apply(quitar_tildes_conservando_enie)

"""###Antes de empezar a correr los codigos del analisis exploratorio correr estas tres pruebas de abajo

"""

columnas_categoricas = ['CRIMINALIDAD', 'ESTADO', 'ETAPA_CASO', 'PA√çS_HECHO', 'DEPARTAMENTO_HECHO', 'MUNICIPIO_HECHO', 'SECCIONAL', 'DELITO', 'GRUPO_DELITO', 'CONSUMADO']

for col in columnas_categoricas:
  if df[col].dtype == "int64" or df[col].dtype == "float64":
    print("HAY UN ERROR EN EL CODIGO, VUELVA A CORRER LOS BLOQUES ANTERIORES NECESARIOS")
    print(f"La columna {df[col]} contiene valores numericos, mas especificamente valores tipo {df[col].dtype} ")

#Con esto verificamos que no hay ningun valor numerico en las columnas categoricas.

contadore = 0
for col1 in df["MUNICIPIO_HECHO"]:
  contadore +=1
  if col1 == "Chim√°":
    print("HAY UN ERROR EN EL CODIGO, VUELVA A CORRER LOS BLOQUES ANTERIORES NECESARIOS")
    print(contadore)
    print(col1)
    print(df.iloc[contadore])
#Con esta prueba damos por hecho de que en las variables categoricas no hay valores con tildes.

columnas_numericas = ["A√ëO_ENTRADA", "A√ëO_DENUNCIA", "A√ëO_HECHOS", "TOTAL_PROCESOS"]

for i in columnas_numericas:
  if df[i].dtype == "object":
    print("HAY UN ERROR EN EL CODIGO, VUELVA A CORRER LOS BLOQUES ANTERIORES NECESARIOS")
    print(f"La columna {df[i]} contiene valores no numericos, mas especificamente valores tipo {df[i].dtype} ")
for z in columnas_numericas:
  if df[z].dtype == "int64" or df[z].dtype == "float64":
    print("HAY UN ERROR EN EL CODIGO, VUELVA A CORRER LOS BLOQUES ANTERIORES NECESARIOS")
    print(f"La columna {df[i]} contiene valores numericos, mas especificamente valores tipo {df[i].dtype} ")
#Con esto verificamos que las columnas numericas no tienen datos tipo object, asi que ya los podemos trabajar estadisticamente sin problema alguno.

"""Para este momento los datos de las variables categoricas deben de estar todas sin tildes, empezar por una mayuscula y ademas no haber ningun espacio entre las comillas.

Ademas los datos de las variables numericas estar en numeros verdaderos (Numeros sin espacios o cualquier otro caracter).

Con esto, doy por finalizado la limpieza profunda de los datos

#LIMPIEZA DE DATOS TERMINADA!!!!!!

#Segunda Fase - Analisis Exploratorio De Los Datos

**Preguntas para empezar el analisis**

¬øQue es la ley 906? ¬øQue tiene que ver en este contexto? ¬øCuando fue creada y por que?

-

¬øCuales son los departamentos con mayores cantidades de casos activos e inactivos?

¬øCuales con municipios con mayores cantidades de casos activos e inactivos?

-

¬øCuales son los departamentos con mayor cantidad de CRIMINALIDAD Y DE NO CRIMINALIDAD?

¬øEn que estado estan la mayoria de casos? ¬øIndagacion o en que?

**Detectando errores en los verdaderos numericos**:
"""

#Una funcion util para ver el maximo valor de una columna:

df["A√ëO_HECHOS"].max()

"""#GRAFICOS"""

# Histograma para las variables num√©ricas

plt.figure(figsize=(8, 6))
sns.histplot(df['A√ëO_ENTRADA'], kde=True,)
plt.title('Registro de entrada de Denuncias realizadas')
plt.xlabel('A√±o de la denuncia')
plt.ylabel('Numero de denuncias')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(df['A√ëO_ENTRADA'],)
plt.title('A√±o en el que se realiz√≥ la denuncia')
plt.ylabel('A√±o de la denuncia')
plt.show()

minimo = df['A√ëO_ENTRADA'].min()
conteo = df['A√ëO_ENTRADA'].value_counts()
print(conteo)

df['TOTAL_PROCESOS'].max()

"""# segundo grupo de preguntas
(Juan Diego)

## ¬øCuales son los departamentos con mayores cantidades de casos activos e inactivos

## ¬øCuales son los municipios con mayores cantidades de casos activos e inactivos
"""

df = data

dep_counts = (df.groupby(['DEPARTAMENTO_HECHO', 'ESTADO']).size().unstack(fill_value=0))

# Ordenar para casos ACTIVOS e INACTIVOS
top_dep_activos   = dep_counts.sort_values('Activo',  ascending=False)
top_dep_inactivos = dep_counts.sort_values('Inactivo', ascending=False)

top10 = dep_counts.sort_values('Inactivo', ascending=False).head(10)

# Barras agrupadas
fig, ax = plt.subplots(figsize=(10, 5))
top10.plot(kind='bar', ax=ax)

ax.set_title('Casos activos e inactivos por departamento (Top 10)')
ax.set_ylabel('Cantidad de casos')
ax.set_xlabel('Departamentos')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

value_counts = df['ETAPA_CASO'].value_counts()
labels = value_counts.index
sizes = value_counts.values
total_casos = sum(sizes)

legend_labels = [f'{label}: {size/total_casos*100 :.1f}%' for label, size in zip(labels, sizes)]

plt.pie(sizes,pctdistance=1.5,radius=1.5, wedgeprops=dict(edgecolor='black', linewidth=1))
plt.title('Distribucion de las Etapas de los casos ', backgroundcolor="0.4", color="1", ma='center')

plt.legend(legend_labels,loc='best',bbox_to_anchor=(0.9, 0, 0.5, 1), title="Etapas del Caso")

plt.show()

# Limpiar nombres de departamentos
df['DEPARTAMENTO_HECHO'] = df['DEPARTAMENTO_HECHO'].str.upper().str.strip()
df['DEPARTAMENTO_HECHO'] = df['DEPARTAMENTO_HECHO'].replace({
    'BOGOT√Å, D. C.': 'CUNDINAMARCA',
    'ARCHIPI√âLAGO DE SAN ANDR√âS, PROVIDENCIA Y SANTA CATALINA': 'SAN ANDR√âS'
})

# Agrupar por departamento
delitos_por_depto = df.groupby('DEPARTAMENTO_HECHO')['TOTAL_PROCESOS'].sum().reset_index()
delitos_por_depto = delitos_por_depto.sort_values('TOTAL_PROCESOS', ascending=False)

# Coordenadas de capitales departamentales
depto_coords = {
    'ANTIOQUIA': [6.2447, -75.5748],  # Medell√≠n
    'VALLE DEL CAUCA': [3.4372, -76.5225],  # Cali
    'CUNDINAMARCA': [4.6097, -74.0817],  # Bogot√°
    'SANTANDER': [7.1193, -73.1227],  # Bucaramanga
    'ATL√ÅNTICO': [10.9639, -74.7964],  # Barranquilla
    'BOL√çVAR': [10.3997, -75.5144],  # Cartagena
    'META': [4.1420, -73.6266],  # Villavicencio
    'HUILA': [2.9345, -75.2809],  # Neiva
    'TOLIMA': [4.4389, -75.2322],  # Ibagu√©
    'NORTE DE SANTANDER': [7.9075, -72.5047],  # C√∫cuta
    'BOYACA': [5.5353, -73.3678],  # Tunja
    'CAUCA': [2.4411, -76.6066],  # Popay√°n
    'NARI√ëO': [1.2136, -77.2811],  # Pasto
    'CESAR': [10.4631, -73.2532],  # Valledupar
    'MAGDALENA': [11.2408, -74.1990],  # Santa Marta
    'CALDAS': [5.0689, -75.5174],  # Manizales
    'RISARALDA': [4.8133, -75.6961],  # Pereira
    'QUINDIO': [4.5350, -75.6756],  # Armenia
    'CAQUET√Å': [1.6146, -75.6062],  # Florencia
    'C√ìRDOBA': [8.7500, -75.8833],  # Monter√≠a
    'SUCRE': [9.3000, -75.4000],  # Sincelejo
    'SAN ANDR√âS': [12.5847, -81.7006],  # San Andr√©s
    'ARAUCA': [7.0903, -70.7617],  # Arauca
    'CASANARE': [5.7278, -71.2578],  # Yopal
    'PUTUMAYO': [1.1486, -76.6304],  # Mocoa
    'LA GUAJIRA': [11.5444, -72.9072],  # Riohacha
}

# Preparar datos para el mapa de calor
heat_data = []
for _, row in delitos_por_depto.iterrows():
    depto = row['DEPARTAMENTO_HECHO']
    if depto in depto_coords:
        lat, lon = depto_coords[depto]
        heat_data.append([lat, lon, row['TOTAL_PROCESOS']])

# Crear mapa centrado en Colombia
m = folium.Map(location=[4.5709, -74.2973], zoom_start=6)

# A√±adir capa de calor
HeatMap(heat_data,
        radius=25,
        blur=15,
        max_zoom=13,
        min_opacity=0.5,
        max_val=delitos_por_depto['TOTAL_PROCESOS'].max()).add_to(m)


# Mostrar el mapa directamente en Colab
display(m)

# Limpiar datos
df['A√ëO_DENUNCIA'] = pd.to_numeric(df['A√ëO_DENUNCIA'], errors='coerce')
df['A√ëO_HECHOS'] = pd.to_numeric(df['A√ëO_HECHOS'], errors='coerce')
df['A√ëO_ENTRADA'] = pd.to_numeric(df['A√ëO_ENTRADA'], errors='coerce')
df_clean = df.dropna(subset=['A√ëO_DENUNCIA', 'A√ëO_HECHOS', 'A√ëO_ENTRADA'])

# Crear figura 3D
fig = plt.figure(figsize=(10,7))
ax = fig.add_subplot(111, projection='3d')

# Definir ejes con datos limpios
x = df['A√ëO_DENUNCIA']
y = df['A√ëO_HECHOS']
z = df['A√ëO_ENTRADA']

# Mapeo de colores
species_colors = {
    'Indagaci√≥n': 'r',
    'Juicio': 'g',
    'Ejecuci√≥n de penas': 'b',
    'Investigaci√≥n': 'm',
    'Terminaci√≥n anticipada': 'y'
}
colors = df['ETAPA_CASO'].map(species_colors)

# Gr√°fico de dispersi√≥n
ax.scatter(x, y, z, c=colors, s=60)

# Etiquetas y t√≠tulo
ax.set_xlabel('A√ëO_DENUNCIA')
ax.set_ylabel('A√ëO_HECHOS')
ax.set_zlabel('A√ëO_ENTRADA')
ax.set_title('Relaci√≥n entre a√±os de denuncia, hechos y entrada')
legend_elements = [Line2D([0], [0], marker='o', color='w', label=key,
                   markerfacecolor=val, markersize=10) for key, val in species_colors.items()]
ax.legend(handles=legend_elements, title='Etapas del Caso')
plt.show()

x , y =df['A√ëO_HECHOS'] , df['A√ëO_ENTRADA']
plt.plot(x,y, alpha = 0.7, marker ="o", linestyle="-")
plt.xlabel('A√±o del delito')
plt.ylabel('A√±o de la denuncia del delito')
plt.grid(True, color="0.1")
plt.show()